{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import *\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from model import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum velocity: 1.89\n",
      "Maximum velocity: 39.32\n",
      "Number of negative velocities: 0\n",
      "[[ 8.05 12.4 ]\n",
      " [ 8.34 12.16]\n",
      " [ 8.12 12.07]\n",
      " [ 6.88 11.44]\n",
      " [ 7.76 11.43]]\n",
      "[0.64919355 0.68585526 0.67274234 0.6013986  0.67891514]\n",
      "Minimum rel velocity: 0.1882521489971347\n",
      "Maximum rel velocity: 2.7437950360288235\n",
      "Minimum log velocity: -1.6699729967118622\n",
      "Maximum log velocity: 1.0093420117490313\n",
      "Minimum normalized velocity: 0.0\n",
      "Maximum normalized velocity: 1.0\n",
      "Normalized midpoint: 0.5\n"
     ]
    }
   ],
   "source": [
    "velocity_path = '../data'\n",
    "normalized_velocities, image_filenames, normalized_midpoint = combine_and_normalize_velocity_files(velocity_path, log_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (316,)\n",
      "strides:  (8,)\n",
      "itemsize:  8\n",
      "aligned:  True\n",
      "contiguous:  True\n",
      "fortran:  True\n",
      "data pointer: 0x21da45ee250\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: float64\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(np.info(normalized_velocities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data\\\\image136.jpg', '../data\\\\image137.jpg', '../data\\\\image138.jpg', '../data\\\\image139.jpg', '../data\\\\image140.jpg']\n",
      "tensor(0.3971)\n",
      "tensor(0.4641)\n",
      "tensor(0.4793)\n",
      "tensor(0.3139)\n",
      "tensor(0.3598)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kxfor\\OneDrive\\Documents\\Projects\\Autonomous-Driving-Project\\path_tracking\\data_processing.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'image': torch.tensor(image, dtype=torch.float), 'velocity': torch.tensor(velocity, dtype=torch.float)}\n"
     ]
    }
   ],
   "source": [
    "image_folder_path = '../data'\n",
    "# Assuming 'normalized_velocities' and 'image_folder_path' are already defined\n",
    "full_dataset = PathFollowingDataset(image_filenames, normalized_velocities, transform=data_transform)\n",
    "\n",
    "print(full_dataset.image_filenames[130:135])\n",
    "# Define the size of the training and validation sets 80/20 train/test split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# Randomly split the dataset into training and validation datasets\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "# print(full_dataset[40:45]['velocity'])\n",
    "for i in range(130,135):\n",
    "    data = full_dataset[i]  # Access each dataset item individually\n",
    "    print(data['velocity']) \n",
    "# print(val_dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNRegressor()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate can be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=75, patience = 20, min_loss_change=0.005):\n",
    "    model.train()  # Set the model to training mode initially\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    epochs_no_improve = 0\n",
    "    last_loss = float('inf')\n",
    "    last_val_loss = float('inf')\n",
    "\n",
    "    # Create a figure for plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_losses = []\n",
    "\n",
    "        # Training Phase\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            inputs = data['image'].to(device)\n",
    "            targets = data['velocity'].to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            if i % 10 == 9:    # Print every 10 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 10:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        # if epoch != 0:\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode for validation\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                inputs = data['image'].to(device)\n",
    "                targets = data['velocity'].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Checkpointing\n",
    "        # defintiely a better way to do this using % but i cant be bothered lol\n",
    "        if epoch == 24 or epoch == 35 or epoch == 45 or epoch == 55 or epoch == 65:\n",
    "            torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "        \n",
    "        if avg_train_loss < 0.03 and avg_val_loss < 0.03:\n",
    "            torch.save(model.state_dict(), f'model_low_loss_epoch_{epoch+1}.pth')\n",
    "            print(f'Model saved with low loss at epoch {epoch+1}')\n",
    "\n",
    "        # check convergence\n",
    "        if abs(avg_train_loss - last_loss) < min_loss_change:\n",
    "            epochs_no_improve+=1\n",
    "        else:\n",
    "            epochs_no_improve=0\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            torch.save(model.state_dict(), f'model_coverged_epoch_{epoch+1}.pth')\n",
    "            print(f'Model has converged and saved at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "        last_loss = avg_train_loss\n",
    "\n",
    "        if avg_val_loss < last_val_loss - min_loss_change:\n",
    "            epochs_no_improve = 0\n",
    "            last_val_loss = avg_val_loss\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Plotting\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.yscale('log')  # Set the y-axis to logarithmic scale\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Log Loss')\n",
    "        plt.title('Training and Validation Loss on Log Scale')\n",
    "        plt.legend()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        # display(plt.gcf())\n",
    "        \n",
    "        plt.pause(0.001)\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(\" \")\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "\n",
    "        model.train()  # Reset to training mode for the next epoch\n",
    "\n",
    "    plt.show()\n",
    "    print('Finished Training and Validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1,  torch.Size([16, 8, 236, 316])\n",
      "c2,  torch.Size([16, 8, 118, 158])\n",
      "p2,  torch.Size([16, 16, 114, 154])\n",
      "c3,  torch.Size([16, 16, 57, 77])\n",
      "fc1,  torch.Size([16, 32, 53, 73])\n",
      "fc2,  torch.Size([16, 120])\n",
      "fc3,  torch.Size([16, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kxfor\\OneDrive\\Documents\\Projects\\Autonomous-Driving-Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1,  torch.Size([16, 8, 236, 316])\n",
      "c2,  torch.Size([16, 8, 118, 158])\n",
      "p2,  torch.Size([16, 16, 114, 154])\n",
      "c3,  torch.Size([16, 16, 57, 77])\n",
      "fc1,  torch.Size([16, 32, 53, 73])\n",
      "fc2,  torch.Size([16, 120])\n",
      "fc3,  torch.Size([16, 84])\n",
      "p1,  torch.Size([16, 8, 236, 316])\n",
      "c2,  torch.Size([16, 8, 118, 158])\n",
      "p2,  torch.Size([16, 16, 114, 154])\n",
      "c3,  torch.Size([16, 16, 57, 77])\n",
      "fc1,  torch.Size([16, 32, 53, 73])\n",
      "fc2,  torch.Size([16, 120])\n",
      "fc3,  torch.Size([16, 84])\n",
      "p1,  torch.Size([16, 8, 236, 316])\n",
      "c2,  torch.Size([16, 8, 118, 158])\n",
      "p2,  torch.Size([16, 16, 114, 154])\n",
      "c3,  torch.Size([16, 16, 57, 77])\n",
      "fc1,  torch.Size([16, 32, 53, 73])\n",
      "fc2,  torch.Size([16, 120])\n",
      "fc3,  torch.Size([16, 84])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_and_validate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m, in \u001b[0;36mtrain_and_validate_model\u001b[1;34m(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs, patience, min_loss_change)\u001b[0m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\kxfor\\OneDrive\\Documents\\Projects\\Autonomous-Driving-Project\\env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kxfor\\OneDrive\\Documents\\Projects\\Autonomous-Driving-Project\\env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_validate_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=75)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
